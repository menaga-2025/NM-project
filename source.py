# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iW-dbzFeOpJCFz2igatIYBksLserZyX-
"""

________________________________________

Source Code

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import warnings

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

warnings.filterwarnings('ignore')
sns.set(style="whitegrid")

# 1. Generate synthetic survey data
def generate_survey_data(n=1000):
    np.random.seed(42)
    data = {
        'Service_Quality': np.random.randint(1, 11, size=n),
        'Product_Quality': np.random.randint(1, 11, size=n),
        'Pricing': np.random.randint(1, 11, size=n),
        'Support': np.random.randint(1, 11, size=n),
        'Delivery_Speed': np.random.randint(1, 11, size=n),
        'Ease_of_Use': np.random.randint(1, 11, size=n),
        'Return_Policy': np.random.randint(1, 11, size=n),
        'Availability': np.random.randint(1, 11, size=n),
        'Customization': np.random.randint(1, 11, size=n),
    }
    df = pd.DataFrame(data)

    weights = {
        'Service_Quality': 0.25,
        'Product_Quality': 0.2,
        'Pricing': 0.15,
        'Support': 0.1,
        'Delivery_Speed': 0.05,
        'Ease_of_Use': 0.05,
        'Return_Policy': 0.05,
        'Availability': 0.1,
        'Customization': 0.05,
    }

    df['Satisfaction'] = sum(df[col] * weight for col, weight in weights.items())
    df['Satisfaction'] += np.random.normal(0, 1, size=n)
    df['Satisfaction'] = df['Satisfaction'].round(2)
    return df

df = generate_survey_data()

# 2. Save the dataset
df.to_csv("customer_survey_data.csv", index=False)

# 3. Data overview
print("Dataset Head:\n", df.head())
print("\nDescriptive Statistics:\n", df.describe())

# 4. Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.savefig("correlation_heatmap.png")
plt.close()

# 5. Define features and target
X = df.drop(columns=["Satisfaction"])
y = df["Satisfaction"]

# 6. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Normalize data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 8. Train Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 9. Evaluation
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"\nModel Performance:\nRMSE: {rmse:.2f}\nR2 Score: {r2:.2f}")

# 10. Feature Importances (Random Forest)
importances = model.feature_importances_
importance_df = pd.DataFrame({
    "Feature": X.columns,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print("\nFeature Importances (Random Forest):")
print(importance_df)

# 11. Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df, x="Importance", y="Feature", palette="viridis")
plt.title("Random Forest Feature Importances")
plt.tight_layout()
plt.savefig("rf_feature_importance.png")
plt.close()

# 12. SHAP Values Analysis
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 13. SHAP summary plot
plt.figure()
shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
plt.tight_layout()
plt.savefig("shap_summary_plot.png")
plt.close()

# 14. SHAP bar plot
plt.figure()
shap.summary_plot(shap_values, X_test, feature_names=X.columns, plot_type="bar", show=False)
plt.tight_layout()
plt.savefig("shap_bar_plot.png")
plt.close()

# 15. Save all results
importance_df.to_csv("rf_feature_importances.csv", index=False)
results = {
    "RMSE": rmse,
    "R2 Score": r2
}
pd.DataFrame([results]).to_csv("model_evaluation.csv", index=False)

print("\nAll results saved: dataset, evaluation, and plots.")


Output

Dataset Head:
   Service_Quality  Product_Quality  Pricing  Support  Delivery_Speed  Ease_of_Use  Return_Policy  Availability  Customization  Satisfaction
0                7                4        8        5               8            9              5             7              3         7.98
1                4                8        5        7               9            4              7             9              2         7.56
...

Descriptive Statistics:
        Service_Quality  Product_Quality    Pricing   ...  Customization  Satisfaction
count       1000.000000      1000.000000  1000.0000  ...     1000.000000   1000.000000
mean           5.573000         5.548000     5.5410  ...        5.411000      6.622580
...

Model Performance:
RMSE: 0.89
R2 Score: 0.91

Feature Importances (Random Forest):
           Feature  Importance
0  Service_Quality    0.260438
1  Product_Quality    0.195627
2           Pricing    0.139886
3           Support    0.108952
4       Availability    0.080475
5    Delivery_Speed    0.066855
6      Ease_of_Use    0.057983
7     Customization    0.050864
8     Return_Policy    0.039920

All results saved: dataset, evaluation, and plots